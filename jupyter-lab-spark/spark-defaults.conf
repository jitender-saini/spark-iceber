spark.sql.extensions=org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions
spark.sql.catalog.local=org.apache.iceberg.spark.SparkCatalog
spark.sql.catalog.local.type=jdbc
spark.sql.catalog.local.uri=jdbc:duckdb:/home/iceberg/catalog/db
spark.sql.catalog.local.io-impl=org.apache.iceberg.aws.s3.S3FileIO
spark.sql.catalog.local.warehouse=s3://datalake/
spark.sql.catalog.local.s3.endpoint=http://minio:9000
spark.sql.defaultCatalog=local
spark.history.fs.logDirectory=/home/iceberg/spark-events

spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem
spark.jars.packages=org.apache.hadoop:hadoop-aws:3.2.0

# Set MinIO endpoint, access key, and secret key
spark.hadoop.fs.s3a.endpoint=http://minio:9000
spark.hadoop.fs.s3a.access.key=minioadmin
spark.hadoop.fs.s3a.secret.key=minioadmin
# Set the file system type to be S3A for the minio endpoint
spark.hadoop.fs.s3a.connection.ssl.enabled=false
# Optional: Disable Path Style Access (useful for some S3 implementations)
spark.hadoop.fs.s3a.path.style.access=true
# AWS credentials provider
spark.hadoop.fs.s3a.aws.credentials.provider=org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider

# Spark SQL Derby logs database
spark.driver.extraJavaOptions=-Dderby.system.home=.logs

# Addressing Warnings
spark.sql.shuffle.partitions=200
spark.sql.autoBroadcastJoinThreshold=-1
spark.hadoop.*.sink.file.enabled=false
spark.hadoop.hive.stats.jdbc.timeout=30
spark.hadoop.hive.stats.retries.wait=3000
spark.hadoop.hive.metastore.schema.verification=true
spark.sql.debug.maxToStringFields=1000
